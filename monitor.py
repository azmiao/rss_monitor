import datetime
import os
from typing import Optional

import feedparser
from rocksdict import Rdict

from yuiChyan import base_db_path, CQEvent, YuiChyan, FunctionException, get_bot
from yuiChyan.service import Service
from yuiChyan.util import RSSParser


sv = Service('rss_monitor')


@sv.on_prefix(('æ·»åŠ RSSè®¢é˜…', 'å¢åŠ RSSè®¢é˜…', 'æ–°å¢RSSè®¢é˜…'))
async def add_rss_url(bot: YuiChyan, ev: CQEvent):
    rss_url = str(ev.message).strip()
    group_id = ev.group_id
    user_id = ev.user_id

    if not rss_url:
        raise FunctionException(ev, f'è¾“å…¥çš„RSSè®¢é˜…URLä¸ºç©º')
    rss_monitor_db = await get_database()
    group_dict = rss_monitor_db.get(group_id, {})
    rss_dict = group_dict.get(user_id, {})
    if rss_url in rss_dict:
        raise FunctionException(ev, f'æ‚¨å·²ç»è®¢é˜…è¿‡è¯¥RSSäº†')
    # æ›´æ–°æ—¶é—´ä¸ºNone
    rss_dict[rss_url] = None
    group_dict[user_id] = rss_dict
    rss_monitor_db[group_id] = group_dict
    rss_monitor_db.close()
    await bot.send(ev, f'å·²æˆåŠŸè®¢é˜…ï¼š{rss_url}', at_sender=True)


@sv.on_match('æŸ¥è¯¢RSSè®¢é˜…')
async def query_rss_url_list(bot: YuiChyan, ev: CQEvent):
    group_id = ev.group_id
    user_id = ev.user_id
    rss_monitor_db = await get_database()
    rss_dict = rss_monitor_db.get(group_id, {}).get(user_id, {})
    if not rss_dict:
        raise FunctionException(ev, f'æ‚¨è¿˜æ²¡æœ‰åœ¨æœ¬ç¾¤è®¢é˜…RSSå‘¢')
    rss_url_list = list(rss_dict.keys())
    rss_monitor_db.close()
    await bot.send(ev, f'æ‚¨å·²è®¢é˜…å¦‚ä¸‹RSSï¼š\n{"\n".join(rss_url_list)}', at_sender=True)


@sv.on_prefix(('åˆ é™¤RSSè®¢é˜…', 'å–æ¶ˆRSSè®¢é˜…'))
async def add_rss_url(bot: YuiChyan, ev: CQEvent):
    rss_url = str(ev.message).strip()
    group_id = ev.group_id
    user_id = ev.user_id

    if not rss_url:
        raise FunctionException(ev, f'è¾“å…¥çš„RSSè®¢é˜…URLä¸ºç©º')
    rss_monitor_db = await get_database()
    group_dict = rss_monitor_db.get(group_id, {})
    rss_dict = group_dict.get(user_id, {})
    if rss_url not in rss_dict:
        raise FunctionException(ev, f'æ‚¨æœªè®¢é˜…è¯¥RSS')
    rss_dict.pop(rss_url)
    group_dict[user_id] = rss_dict
    rss_monitor_db[group_id] = group_dict
    rss_monitor_db.close()
    await bot.send(ev, f'å·²æˆåŠŸåˆ é™¤è®¢é˜…ï¼š{rss_url}', at_sender=True)


@sv.scheduled_job(minute='*/1')
async def monitor_schedule():
    bot = get_bot()
    rss_monitor_db = await get_database()
    for group_id in rss_monitor_db.keys():
        group_data: dict = rss_monitor_db.get(group_id, {})
        for user_id, rss_dict in group_data.items():
            # ç¬¬ä¸€æ¬¡è®¢é˜…åupdate_timeä¸ºNone
            for rss_url, update_time in rss_dict.items():
                # try:
                # æ£€æµ‹ RSS æ˜¯å¦æœ‰æ›´æ–°
                new_update_time, new_entries = await check_rss(rss_url, update_time)
                # æœ‰æ›´æ–°
                if new_entries:
                    # æ›´æ–°æ•°æ®åº“é‡Œçš„æ›´æ–°æ—¶é—´
                    rss_dict[rss_url] = new_update_time
                    rss_monitor_db[group_id][user_id] = rss_dict
                    await bot.send_group_msg(group_id=group_id, message=format_entries_message(new_entries))
                # except Exception as e:
                #     print(f"[ERROR] ç›‘æ§ RSS {rss_url} å¤±è´¥: {e}")


async def get_database() -> Rdict:
    """
    ç›‘æ§ä¿¡æ¯æ•°æ®åº“
    """
    rss_monitor_db = Rdict(os.path.join(base_db_path, 'rss_monitor.db'))
    return rss_monitor_db


async def check_rss(rss_url: str, update_time: Optional[str]):
    """
    æ£€æµ‹ RSS æ˜¯å¦æœ‰æ–°å†…å®¹
    :param rss_url: RSS é“¾æ¥
    :param update_time: æ•°æ®åº“ä¸­è®°å½•çš„ä¸Šæ¬¡æ›´æ–°æ—¶é—´ï¼ˆå¯èƒ½æ˜¯ Noneï¼‰
    :return: (æ–°çš„æ›´æ–°æ—¶é—´, æ–°çš„æ¡ç›®åˆ—è¡¨)
    """
    parser = RSSParser(rss_url)
    feed = parser.parse_feed()

    if not feed.entries:
        return update_time, []

    # æ‰¾å‡ºåˆ—è¡¨é‡Œæœ€æ–°çš„æ—¶é—´
    latest_time_dt = None
    latest_time_str = None
    for entry in feed.entries:
        entry_dt = parse_datetime(entry.published)
        sv.logger.info(entry.published)
        sv.logger.info(entry_dt)
        if entry_dt and (latest_time_dt is None or entry_dt > latest_time_dt):
            latest_time_dt = entry_dt
            latest_time_str = entry.published

    new_entries = []
    # ç¬¬ä¸€æ¬¡è®¢é˜…ï¼Œè¿”å›å…¨éƒ¨
    if update_time is None:
        new_entries = feed.entries
    else:
        old_time_dt = parse_datetime(update_time)
        for entry in feed.entries:
            entry_dt = parse_datetime(entry.published)
            if entry_dt and entry_dt > old_time_dt:
                new_entries.append(entry)

    return latest_time_str, new_entries


def parse_datetime(dt_str: str) -> datetime.datetime | None:
    """è§£æ RSS æ—¥æœŸå­—ç¬¦ä¸²ï¼Œè¿”å› datetimeï¼ˆUTCæ ‡å‡†åŒ–ï¼‰"""
    if not dt_str:
        return None

    # ä¼˜å…ˆç”¨ ISO8601 æ ¼å¼è§£æï¼Œä¾‹å¦‚ 2025-11-20T22:48:23+08:00
    try:
        return datetime.datetime.fromisoformat(dt_str)
    except ValueError:
        pass

    # å°è¯• feedparser çš„è§£æåŠŸèƒ½ï¼ˆæ”¯æŒ RFC822 ç­‰ï¼‰
    try:
        parsed = feedparser.parse(dt_str)
        if hasattr(parsed, "updated_parsed") and parsed.updated_parsed:
            return datetime.datetime(*parsed.updated_parsed[:6])
    except Exception:
        pass

    return None


def format_entries_message(entries, limit: int = 5):
    msgs = []
    total = len(entries)
    for e in entries[:limit]:
        msgs.append(f"ğŸ“¢ {e.title}\nğŸ”— {e.link}\nğŸ•’ {e.published}")
    if total > limit:
        msgs.append(f"â€¦è¿˜æœ‰ {total - limit} æ¡æ–°å†…å®¹æœªæ˜¾ç¤º")
    return "\n\n".join(msgs)
